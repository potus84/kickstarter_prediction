{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Tuning on train set (remove the post features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/tai/Projects/kickstarter_prediction/')\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import xgboost\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = join(DATA_SPLIT_ROOT, 'train.csv')\n",
    "test_set = join(DATA_SPLIT_ROOT, 'test.csv')\n",
    "\n",
    "train = pd.read_csv(train_set, encoding='latin1', low_memory=True)\n",
    "test = pd.read_csv(test_set, encoding='latin1', low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_features = ['pledged_per_backer', 'required_backers', 'required_backers_per_day']\n",
    "train = train.drop(columns=['pledged_per_backer','required_backers','required_backers_per_day'])\n",
    "test = test.drop(post_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.drop(['success'], axis=1)\n",
    "train_targets = train['success']\n",
    "\n",
    "test_features = test.drop(['success'], axis=1)\n",
    "test_targets = test['success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the best n_estimator for the XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.366554</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.368489</td>\n",
       "      <td>0.004747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.354418</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.356450</td>\n",
       "      <td>0.001496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.350135</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.350782</td>\n",
       "      <td>0.003484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.347629</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.348004</td>\n",
       "      <td>0.003272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.346954</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.347788</td>\n",
       "      <td>0.003182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.345765</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.346668</td>\n",
       "      <td>0.003628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.342791</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.343951</td>\n",
       "      <td>0.001896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.341723</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.342051</td>\n",
       "      <td>0.001751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.339844</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.341284</td>\n",
       "      <td>0.001932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.339009</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.340070</td>\n",
       "      <td>0.000903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.337465</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.338321</td>\n",
       "      <td>0.002024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.336247</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.337382</td>\n",
       "      <td>0.001925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.335116</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.335935</td>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.333747</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.334906</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.332847</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.333898</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.331745</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.001252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.330698</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.332244</td>\n",
       "      <td>0.001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.329888</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.331339</td>\n",
       "      <td>0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.329024</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.330413</td>\n",
       "      <td>0.001356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.328696</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.330107</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.327691</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.329121</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.326896</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.328238</td>\n",
       "      <td>0.001267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.326177</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.327859</td>\n",
       "      <td>0.001661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.325384</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.326920</td>\n",
       "      <td>0.001732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.324547</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.326166</td>\n",
       "      <td>0.001566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.324187</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.325559</td>\n",
       "      <td>0.001471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.323736</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.325326</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.323001</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.324697</td>\n",
       "      <td>0.001485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.322668</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.324396</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.322169</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.323995</td>\n",
       "      <td>0.001338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.289030</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.304092</td>\n",
       "      <td>0.001397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.289019</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.304165</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.288991</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.304131</td>\n",
       "      <td>0.001416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.288882</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.304161</td>\n",
       "      <td>0.001373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.288868</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.304148</td>\n",
       "      <td>0.001334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.288842</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.304126</td>\n",
       "      <td>0.001329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.288784</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.304100</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.288720</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.303984</td>\n",
       "      <td>0.001393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.288658</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.303976</td>\n",
       "      <td>0.001423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.288629</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.303967</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.288591</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.303971</td>\n",
       "      <td>0.001356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.288588</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.303932</td>\n",
       "      <td>0.001336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.288563</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.303928</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.288521</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.303915</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.288459</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.303885</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.288436</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.303885</td>\n",
       "      <td>0.001291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.288434</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.303816</td>\n",
       "      <td>0.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.288336</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.303782</td>\n",
       "      <td>0.001277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.288350</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.303778</td>\n",
       "      <td>0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.288335</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.303786</td>\n",
       "      <td>0.001295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.288295</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.303825</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.288265</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.303860</td>\n",
       "      <td>0.001205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.288247</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.303773</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.288201</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.303769</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.288194</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.303691</td>\n",
       "      <td>0.001129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.288146</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.303691</td>\n",
       "      <td>0.001197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.288130</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.303657</td>\n",
       "      <td>0.001296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.288026</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.303670</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.287986</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.303670</td>\n",
       "      <td>0.001332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.287922</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.303618</td>\n",
       "      <td>0.001313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.366554         0.005534         0.368489        0.004747\n",
       "1            0.354418         0.001786         0.356450        0.001496\n",
       "2            0.350135         0.002585         0.350782        0.003484\n",
       "3            0.347629         0.003125         0.348004        0.003272\n",
       "4            0.346954         0.001595         0.347788        0.003182\n",
       "5            0.345765         0.002496         0.346668        0.003628\n",
       "6            0.342791         0.001176         0.343951        0.001896\n",
       "7            0.341723         0.001115         0.342051        0.001751\n",
       "8            0.339844         0.000706         0.341284        0.001932\n",
       "9            0.339009         0.000994         0.340070        0.000903\n",
       "10           0.337465         0.000849         0.338321        0.002024\n",
       "11           0.336247         0.001025         0.337382        0.001925\n",
       "12           0.335116         0.000532         0.335935        0.000963\n",
       "13           0.333747         0.001024         0.334906        0.001364\n",
       "14           0.332847         0.001093         0.333898        0.001240\n",
       "15           0.331745         0.001237         0.333001        0.001252\n",
       "16           0.330698         0.001180         0.332244        0.001058\n",
       "17           0.329888         0.001001         0.331339        0.001377\n",
       "18           0.329024         0.000991         0.330413        0.001356\n",
       "19           0.328696         0.001007         0.330107        0.001346\n",
       "20           0.327691         0.001278         0.329121        0.001421\n",
       "21           0.326896         0.000929         0.328238        0.001267\n",
       "22           0.326177         0.000777         0.327859        0.001661\n",
       "23           0.325384         0.000504         0.326920        0.001732\n",
       "24           0.324547         0.000807         0.326166        0.001566\n",
       "25           0.324187         0.000722         0.325559        0.001471\n",
       "26           0.323736         0.000771         0.325326        0.001507\n",
       "27           0.323001         0.000894         0.324697        0.001485\n",
       "28           0.322668         0.000863         0.324396        0.001396\n",
       "29           0.322169         0.000966         0.323995        0.001338\n",
       "..                ...              ...              ...             ...\n",
       "382          0.289030         0.000561         0.304092        0.001397\n",
       "383          0.289019         0.000576         0.304165        0.001408\n",
       "384          0.288991         0.000579         0.304131        0.001416\n",
       "385          0.288882         0.000597         0.304161        0.001373\n",
       "386          0.288868         0.000613         0.304148        0.001334\n",
       "387          0.288842         0.000604         0.304126        0.001329\n",
       "388          0.288784         0.000602         0.304100        0.001381\n",
       "389          0.288720         0.000559         0.303984        0.001393\n",
       "390          0.288658         0.000538         0.303976        0.001423\n",
       "391          0.288629         0.000587         0.303967        0.001374\n",
       "392          0.288591         0.000599         0.303971        0.001356\n",
       "393          0.288588         0.000585         0.303932        0.001336\n",
       "394          0.288563         0.000574         0.303928        0.001313\n",
       "395          0.288521         0.000546         0.303915        0.001310\n",
       "396          0.288459         0.000552         0.303885        0.001310\n",
       "397          0.288436         0.000554         0.303885        0.001291\n",
       "398          0.288434         0.000512         0.303816        0.001317\n",
       "399          0.288336         0.000478         0.303782        0.001277\n",
       "400          0.288350         0.000508         0.303778        0.001246\n",
       "401          0.288335         0.000530         0.303786        0.001295\n",
       "402          0.288295         0.000520         0.303825        0.001220\n",
       "403          0.288265         0.000525         0.303860        0.001205\n",
       "404          0.288247         0.000526         0.303773        0.001135\n",
       "405          0.288201         0.000509         0.303769        0.001120\n",
       "406          0.288194         0.000508         0.303691        0.001129\n",
       "407          0.288146         0.000525         0.303691        0.001197\n",
       "408          0.288130         0.000537         0.303657        0.001296\n",
       "409          0.288026         0.000485         0.303670        0.001318\n",
       "410          0.287986         0.000476         0.303670        0.001332\n",
       "411          0.287922         0.000487         0.303618        0.001313\n",
       "\n",
       "[412 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    scale_pos_weight=1,\n",
    "    n_jobs=-1,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(train_features, label=train_targets)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune best min_child_weight and max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Best score:  0.6962312098893052\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=411,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        scale_pos_weight=1,\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test1,\n",
    "                        scoring='accuracy', n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch1.fit(train_features,train_targets)\n",
    "print('Best param: ', gsearch1.best_params_)\n",
    "print('Best score: ', gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param:  {'max_depth': 6, 'min_child_weight': 2}\n",
      "Best score:  0.6965499418529526\n"
     ]
    }
   ],
   "source": [
    "# Carefully take a look to neighbor area\n",
    "param_test2 = {\n",
    " 'max_depth':[4, 5, 6],\n",
    " 'min_child_weight':[1, 2, 3]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=411,\n",
    "        max_depth=9,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test2,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch2.fit(train_features,train_targets)\n",
    "print('Best param: ', gsearch2.best_params_)\n",
    "print('Best score: ', gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param:  {'min_child_weight': 10}\n",
      "Best score:  0.6966016281173278\n"
     ]
    }
   ],
   "source": [
    "# Try with the weight greater than 10\n",
    "param_test2b = {\n",
    " 'min_child_weight':[10, 11, 12]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=411,\n",
    "        max_depth=6,\n",
    "        min_child_weight=2,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch2b = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test2b,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch2b.fit(train_features,train_targets)\n",
    "print('Best param: ', gsearch2b.best_params_)\n",
    "print('Best score: ', gsearch2b.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'min_child_weight': 9}\n",
      "Run {} best score:  0.6968169875522247\n"
     ]
    }
   ],
   "source": [
    "# param_test2c = {\n",
    "#  'min_child_weight':[9, 10, 11]\n",
    "# }\n",
    "# xgb = XGBClassifier(\n",
    "#         learning_rate =0.1,\n",
    "#         n_estimators=411,\n",
    "#         max_depth=6,\n",
    "#         min_child_weight=5,\n",
    "#         gamma=0,\n",
    "#         subsample=0.8,\n",
    "#         colsample_bytree=0.8,\n",
    "#         objective= 'binary:logistic',\n",
    "#         n_jobs=8,\n",
    "#         scale_pos_weight=1,\n",
    "#         seed=0)\n",
    "# five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# gsearch2c = GridSearchCV(estimator = xgb,\n",
    "#                         param_grid = param_test2c,\n",
    "#                         scoring='accuracy',n_jobs=-1,\n",
    "#                         cv=five_folds)\n",
    "# gsearch2c.fit(train_features,train_targets)\n",
    "# print('Run {} best param: ', gsearch2c.best_params_)\n",
    "# print('Run {} best score: ', gsearch2c.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'gamma': 0.0}\n",
      "Run {} best score:  0.6966016281173278\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=411,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test3,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch3.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch3.best_params_)\n",
    "print('Run {} best score: ', gsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalibrate the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.361585</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.363906</td>\n",
       "      <td>0.005019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.349196</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.351678</td>\n",
       "      <td>0.004441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.343806</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.345001</td>\n",
       "      <td>0.002332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.340888</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.342210</td>\n",
       "      <td>0.001419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.340704</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.342167</td>\n",
       "      <td>0.002021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.339145</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.341185</td>\n",
       "      <td>0.001816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.336930</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.338816</td>\n",
       "      <td>0.001931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.336177</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.337649</td>\n",
       "      <td>0.002068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.335360</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.336930</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.333099</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.334716</td>\n",
       "      <td>0.001268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.331751</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.333910</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.330620</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.332692</td>\n",
       "      <td>0.001599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.329482</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.331292</td>\n",
       "      <td>0.001371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.328084</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.330241</td>\n",
       "      <td>0.001293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.327058</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.329108</td>\n",
       "      <td>0.001172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.326123</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.328272</td>\n",
       "      <td>0.001066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.325445</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.327880</td>\n",
       "      <td>0.001068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.324358</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.326506</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.323353</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.325692</td>\n",
       "      <td>0.000881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.322962</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.325154</td>\n",
       "      <td>0.000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.322095</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.324276</td>\n",
       "      <td>0.001049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.321567</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.323827</td>\n",
       "      <td>0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.320953</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.323332</td>\n",
       "      <td>0.001512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.320199</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.322484</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.319568</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.321967</td>\n",
       "      <td>0.001439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.319273</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.321937</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.318870</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.321708</td>\n",
       "      <td>0.001494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.318155</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.001770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.317978</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.320692</td>\n",
       "      <td>0.001871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.317314</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.320209</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>0.282361</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0.282366</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.302812</td>\n",
       "      <td>0.001256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0.282306</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.302843</td>\n",
       "      <td>0.001208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.282280</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.302813</td>\n",
       "      <td>0.001181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.282247</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.302843</td>\n",
       "      <td>0.001152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.282191</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.302830</td>\n",
       "      <td>0.001175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.282132</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.302886</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.282087</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.302912</td>\n",
       "      <td>0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>0.282039</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.302938</td>\n",
       "      <td>0.001091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.282009</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.302972</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>0.281986</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.303024</td>\n",
       "      <td>0.001059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>0.281925</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.303097</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.281882</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.303011</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.281777</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.303028</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.281750</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.303041</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.281697</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>0.001081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.281616</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.303075</td>\n",
       "      <td>0.001114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.281558</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.303062</td>\n",
       "      <td>0.001158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.281543</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.303062</td>\n",
       "      <td>0.001189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.281517</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.302981</td>\n",
       "      <td>0.001206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.281449</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.302912</td>\n",
       "      <td>0.001150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.281430</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.302950</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.281448</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.302972</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.281386</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.302920</td>\n",
       "      <td>0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.281290</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.302873</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.281292</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.302942</td>\n",
       "      <td>0.001120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.281215</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.302903</td>\n",
       "      <td>0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.281179</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.302916</td>\n",
       "      <td>0.001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.281156</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.302834</td>\n",
       "      <td>0.001043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.281094</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.302765</td>\n",
       "      <td>0.001112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.361585         0.004911         0.363906        0.005019\n",
       "1            0.349196         0.003559         0.351678        0.004441\n",
       "2            0.343806         0.002168         0.345001        0.002332\n",
       "3            0.340888         0.001989         0.342210        0.001419\n",
       "4            0.340704         0.002249         0.342167        0.002021\n",
       "5            0.339145         0.002321         0.341185        0.001816\n",
       "6            0.336930         0.001328         0.338816        0.001931\n",
       "7            0.336177         0.001100         0.337649        0.002068\n",
       "8            0.335360         0.000757         0.336930        0.001584\n",
       "9            0.333099         0.000802         0.334716        0.001268\n",
       "10           0.331751         0.001035         0.333910        0.001256\n",
       "11           0.330620         0.001111         0.332692        0.001599\n",
       "12           0.329482         0.000842         0.331292        0.001371\n",
       "13           0.328084         0.000969         0.330241        0.001293\n",
       "14           0.327058         0.000860         0.329108        0.001172\n",
       "15           0.326123         0.000603         0.328272        0.001066\n",
       "16           0.325445         0.000910         0.327880        0.001068\n",
       "17           0.324358         0.001176         0.326506        0.000925\n",
       "18           0.323353         0.001055         0.325692        0.000881\n",
       "19           0.322962         0.000664         0.325154        0.000914\n",
       "20           0.322095         0.000824         0.324276        0.001049\n",
       "21           0.321567         0.000733         0.323827        0.001246\n",
       "22           0.320953         0.000756         0.323332        0.001512\n",
       "23           0.320199         0.000682         0.322484        0.001618\n",
       "24           0.319568         0.000954         0.321967        0.001439\n",
       "25           0.319273         0.000878         0.321937        0.001412\n",
       "26           0.318870         0.000742         0.321708        0.001494\n",
       "27           0.318155         0.000837         0.321101        0.001770\n",
       "28           0.317978         0.000799         0.320692        0.001871\n",
       "29           0.317314         0.001036         0.320209        0.001488\n",
       "..                ...              ...              ...             ...\n",
       "462          0.282361         0.000765         0.302800        0.001326\n",
       "463          0.282366         0.000779         0.302812        0.001256\n",
       "464          0.282306         0.000789         0.302843        0.001208\n",
       "465          0.282280         0.000766         0.302813        0.001181\n",
       "466          0.282247         0.000810         0.302843        0.001152\n",
       "467          0.282191         0.000847         0.302830        0.001175\n",
       "468          0.282132         0.000789         0.302886        0.001098\n",
       "469          0.282087         0.000796         0.302912        0.001108\n",
       "470          0.282039         0.000783         0.302938        0.001091\n",
       "471          0.282009         0.000760         0.302972        0.001018\n",
       "472          0.281986         0.000734         0.303024        0.001059\n",
       "473          0.281925         0.000738         0.303097        0.001054\n",
       "474          0.281882         0.000710         0.303011        0.001012\n",
       "475          0.281777         0.000649         0.303028        0.001025\n",
       "476          0.281750         0.000667         0.303041        0.001012\n",
       "477          0.281697         0.000645         0.303006        0.001081\n",
       "478          0.281616         0.000703         0.303075        0.001114\n",
       "479          0.281558         0.000721         0.303062        0.001158\n",
       "480          0.281543         0.000711         0.303062        0.001189\n",
       "481          0.281517         0.000716         0.302981        0.001206\n",
       "482          0.281449         0.000769         0.302912        0.001150\n",
       "483          0.281430         0.000791         0.302950        0.001111\n",
       "484          0.281448         0.000829         0.302972        0.001054\n",
       "485          0.281386         0.000825         0.302920        0.001063\n",
       "486          0.281290         0.000827         0.302873        0.001118\n",
       "487          0.281292         0.000805         0.302942        0.001120\n",
       "488          0.281215         0.000775         0.302903        0.001067\n",
       "489          0.281179         0.000772         0.302916        0.001061\n",
       "490          0.281156         0.000796         0.302834        0.001043\n",
       "491          0.281094         0.000762         0.302765        0.001112\n",
       "\n",
       "[492 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=6,\n",
    "    min_child_weight=10,\n",
    "    gamma=0.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=8,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(train_features, label=train_targets)\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the colsample_bytree and subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'colsample_bytree': 0.7, 'subsample': 0.7}\n",
      "Run {} best score:  0.6974243011586337\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    " 'subsample':[i/10.0 for i in range(6,10)] \n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=491,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test4,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch4.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch4.best_params_)\n",
    "print('Run {} best score: ', gsearch4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'colsample_bytree': 0.7, 'subsample': 0.7}\n",
      "Run {} best score:  0.6974243011586337\n"
     ]
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'colsample_bytree':[0.65, 0.7, 0.75],\n",
    " 'subsample':[0.65, 0.7, 0.75] \n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=491,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test5,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch5.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch5.best_params_)\n",
    "print('Run {} best score: ', gsearch5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'reg_alpha': 0}\n",
      "Run {} best score:  0.6974243011586337\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=491,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        gamma=0.0,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test6,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch6.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch6.best_params_)\n",
    "print('Run {} best score: ', gsearch6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'reg_alpha': 0}\n",
      "Run {} best score:  0.6974243011586337\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=491,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        gamma=0,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test7,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch7.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch7.best_params_)\n",
    "print('Run {} best score: ', gsearch7.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 69.9353%\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=491,\n",
    "        max_depth=6,\n",
    "        min_child_weight=10,\n",
    "        gamma=0,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        reg_alpha=1e-5,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0\n",
    "    )\n",
    "model = xgb.fit(train_features, train_targets)\n",
    "y_pred = model.predict(test_features)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test_targets, y_pred)\n",
    "print('Accuracy %.4f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
