{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Tuning on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/tai/Projects/kickstarter_prediction/')\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import xgboost\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = join(DATA_SPLIT_ROOT, 'train.csv')\n",
    "test_set = join(DATA_SPLIT_ROOT, 'test.csv')\n",
    "\n",
    "train = pd.read_csv(train_set, encoding='latin1', low_memory=True)\n",
    "test = pd.read_csv(test_set, encoding='latin1', low_memory=True)\n",
    "\n",
    "train_features = train.drop(['success'], axis=1)\n",
    "train_targets = train['success']\n",
    "\n",
    "test_features = test.drop(['success'], axis=1)\n",
    "test_targets = test['success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the best n_estimator for the XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235662</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.236628</td>\n",
       "      <td>0.005942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.229322</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>0.230758</td>\n",
       "      <td>0.005505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.226116</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.228014</td>\n",
       "      <td>0.003794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.223574</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.225322</td>\n",
       "      <td>0.001181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.223233</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.224749</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.222071</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.223875</td>\n",
       "      <td>0.001291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.221235</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.223013</td>\n",
       "      <td>0.001188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.220824</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.222751</td>\n",
       "      <td>0.001205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.220218</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.221988</td>\n",
       "      <td>0.001676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.220205</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.221786</td>\n",
       "      <td>0.001556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.219869</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.221639</td>\n",
       "      <td>0.001870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.219215</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.220899</td>\n",
       "      <td>0.001481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.218734</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.220386</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.218334</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.219645</td>\n",
       "      <td>0.001233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.217885</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.219490</td>\n",
       "      <td>0.001242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.217497</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.219033</td>\n",
       "      <td>0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.216994</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.218460</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.216555</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.217982</td>\n",
       "      <td>0.000953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.216114</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.217668</td>\n",
       "      <td>0.001201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.215694</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.217138</td>\n",
       "      <td>0.001145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.215297</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.216785</td>\n",
       "      <td>0.001346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.215078</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.216488</td>\n",
       "      <td>0.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.214699</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.216286</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.214335</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.215807</td>\n",
       "      <td>0.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.215424</td>\n",
       "      <td>0.001196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.213443</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.214933</td>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.213025</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.214541</td>\n",
       "      <td>0.000935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.212695</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.214171</td>\n",
       "      <td>0.001163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.212342</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.213917</td>\n",
       "      <td>0.000994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.212005</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.213507</td>\n",
       "      <td>0.001158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.180539</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.191623</td>\n",
       "      <td>0.001386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.180523</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.191644</td>\n",
       "      <td>0.001385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.180506</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.191571</td>\n",
       "      <td>0.001442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.180483</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.191558</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.180411</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.191506</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.180380</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.191498</td>\n",
       "      <td>0.001491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.180339</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.191506</td>\n",
       "      <td>0.001483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.180260</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.191424</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.180199</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.191347</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.180192</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.191360</td>\n",
       "      <td>0.001488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.180149</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.191433</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.180102</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.191442</td>\n",
       "      <td>0.001491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.180056</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.179967</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.191295</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.179905</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.191286</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.179857</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.191304</td>\n",
       "      <td>0.001431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.179784</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.191192</td>\n",
       "      <td>0.001425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.179729</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.191175</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.179705</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.191149</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.179679</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.191131</td>\n",
       "      <td>0.001382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.179607</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.191196</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.179540</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.191218</td>\n",
       "      <td>0.001354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.179504</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.191222</td>\n",
       "      <td>0.001321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.179444</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.191153</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.179401</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.191136</td>\n",
       "      <td>0.001273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.179383</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.191084</td>\n",
       "      <td>0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.179357</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.191020</td>\n",
       "      <td>0.001252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.179320</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.191007</td>\n",
       "      <td>0.001229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.179295</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.191011</td>\n",
       "      <td>0.001246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.179275</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.190964</td>\n",
       "      <td>0.001257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.235662         0.006116         0.236628        0.005942\n",
       "1            0.229322         0.005648         0.230758        0.005505\n",
       "2            0.226116         0.003752         0.228014        0.003794\n",
       "3            0.223574         0.001151         0.225322        0.001181\n",
       "4            0.223233         0.000892         0.224749        0.001409\n",
       "5            0.222071         0.001192         0.223875        0.001291\n",
       "6            0.221235         0.000390         0.223013        0.001188\n",
       "7            0.220824         0.000828         0.222751        0.001205\n",
       "8            0.220218         0.000905         0.221988        0.001676\n",
       "9            0.220205         0.000520         0.221786        0.001556\n",
       "10           0.219869         0.000574         0.221639        0.001870\n",
       "11           0.219215         0.000731         0.220899        0.001481\n",
       "12           0.218734         0.000626         0.220386        0.001543\n",
       "13           0.218334         0.000734         0.219645        0.001233\n",
       "14           0.217885         0.000641         0.219490        0.001242\n",
       "15           0.217497         0.000545         0.219033        0.000873\n",
       "16           0.216994         0.000552         0.218460        0.001045\n",
       "17           0.216555         0.000584         0.217982        0.000953\n",
       "18           0.216114         0.000608         0.217668        0.001201\n",
       "19           0.215694         0.000624         0.217138        0.001145\n",
       "20           0.215297         0.000472         0.216785        0.001346\n",
       "21           0.215078         0.000489         0.216488        0.001317\n",
       "22           0.214699         0.000477         0.216286        0.001012\n",
       "23           0.214335         0.000362         0.215807        0.001127\n",
       "24           0.213983         0.000278         0.215424        0.001196\n",
       "25           0.213443         0.000337         0.214933        0.000850\n",
       "26           0.213025         0.000333         0.214541        0.000935\n",
       "27           0.212695         0.000315         0.214171        0.001163\n",
       "28           0.212342         0.000508         0.213917        0.000994\n",
       "29           0.212005         0.000626         0.213507        0.001158\n",
       "..                ...              ...              ...             ...\n",
       "351          0.180539         0.001095         0.191623        0.001386\n",
       "352          0.180523         0.001115         0.191644        0.001385\n",
       "353          0.180506         0.001119         0.191571        0.001442\n",
       "354          0.180483         0.001115         0.191558        0.001452\n",
       "355          0.180411         0.001123         0.191506        0.001513\n",
       "356          0.180380         0.001102         0.191498        0.001491\n",
       "357          0.180339         0.001111         0.191506        0.001483\n",
       "358          0.180260         0.001104         0.191424        0.001513\n",
       "359          0.180199         0.001090         0.191347        0.001464\n",
       "360          0.180192         0.001128         0.191360        0.001488\n",
       "361          0.180149         0.001122         0.191433        0.001462\n",
       "362          0.180102         0.001136         0.191442        0.001491\n",
       "363          0.180056         0.001155         0.191420        0.001449\n",
       "364          0.179967         0.001129         0.191295        0.001422\n",
       "365          0.179905         0.001108         0.191286        0.001446\n",
       "366          0.179857         0.001119         0.191304        0.001431\n",
       "367          0.179784         0.001127         0.191192        0.001425\n",
       "368          0.179729         0.001127         0.191175        0.001402\n",
       "369          0.179705         0.001139         0.191149        0.001452\n",
       "370          0.179679         0.001155         0.191131        0.001382\n",
       "371          0.179607         0.001152         0.191196        0.001351\n",
       "372          0.179540         0.001184         0.191218        0.001354\n",
       "373          0.179504         0.001192         0.191222        0.001321\n",
       "374          0.179444         0.001206         0.191153        0.001282\n",
       "375          0.179401         0.001207         0.191136        0.001273\n",
       "376          0.179383         0.001189         0.191084        0.001246\n",
       "377          0.179357         0.001153         0.191020        0.001252\n",
       "378          0.179320         0.001153         0.191007        0.001229\n",
       "379          0.179295         0.001146         0.191011        0.001246\n",
       "380          0.179275         0.001142         0.190964        0.001257\n",
       "\n",
       "[381 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    scale_pos_weight=1,\n",
    "    n_jobs=-1,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(train_features, label=train_targets)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=10, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune best min_child_weight and max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best param:  {'max_depth': 9, 'min_child_weight': 5}\n",
      "Best score:  0.812417625016152\n"
     ]
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=380,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test1,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch1.fit(train_features,train_targets)\n",
    "print('Best param: ', gsearch1.best_params_)\n",
    "print('Best score: ', gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'max_depth': 10, 'min_child_weight': 5}\n",
      "Run {} best score:  0.8124262393935479\n"
     ]
    }
   ],
   "source": [
    "# Carefully take a look to neighbor area\n",
    "param_test2 = {\n",
    " 'max_depth':[8,9,10],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=380,\n",
    "        max_depth=9,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test2,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch2.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch2.best_params_)\n",
    "print('Run {} best score: ', gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'min_child_weight': 5}\n",
      "Run {} best score:  0.8124262393935479\n"
     ]
    }
   ],
   "source": [
    "# Try with the weight greater than 10\n",
    "param_test2b = {\n",
    " 'min_child_weight':[5, 6, 8, 10, 12]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=380,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch2b = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test2b,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch2b.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch2b.best_params_)\n",
    "print('Run {} best score: ', gsearch2b.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'gamma': 0.0}\n",
      "Run {} best score:  0.8124262393935479\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=380,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test3,\n",
    "                        scoring='accuracy',n_jobs=-1,\n",
    "                        cv=five_folds)\n",
    "gsearch3.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch3.best_params_)\n",
    "print('Run {} best score: ', gsearch3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalibrate the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213419</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.220864</td>\n",
       "      <td>0.002292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206908</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.214756</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204342</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.213219</td>\n",
       "      <td>0.000707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.202497</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.211242</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.201245</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.210544</td>\n",
       "      <td>0.001329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.200197</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.209876</td>\n",
       "      <td>0.001262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.199526</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.208912</td>\n",
       "      <td>0.000577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.198771</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.208550</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.197932</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.207882</td>\n",
       "      <td>0.000807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.197547</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.207710</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.196884</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.207271</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.196230</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.001113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.195826</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.206680</td>\n",
       "      <td>0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.195235</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.206116</td>\n",
       "      <td>0.001158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.194792</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.205914</td>\n",
       "      <td>0.001319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.194234</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.205561</td>\n",
       "      <td>0.001290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.193712</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.205087</td>\n",
       "      <td>0.001166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.193344</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.204768</td>\n",
       "      <td>0.001259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.192753</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.204544</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.192227</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.204122</td>\n",
       "      <td>0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.191666</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.203937</td>\n",
       "      <td>0.001055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.191278</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.203648</td>\n",
       "      <td>0.001073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.190869</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.203347</td>\n",
       "      <td>0.001203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.190461</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.202942</td>\n",
       "      <td>0.001305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.189962</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.202597</td>\n",
       "      <td>0.001243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.189516</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.202287</td>\n",
       "      <td>0.001309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.189312</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.201951</td>\n",
       "      <td>0.001340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.188756</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.201826</td>\n",
       "      <td>0.001517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.188434</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.201348</td>\n",
       "      <td>0.001582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.188088</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.201391</td>\n",
       "      <td>0.001531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.138505</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.002103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.138505</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.188224</td>\n",
       "      <td>0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.138329</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.188164</td>\n",
       "      <td>0.002082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.138194</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.188147</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.138078</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.188151</td>\n",
       "      <td>0.002164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.138011</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.188181</td>\n",
       "      <td>0.002217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.137940</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.188233</td>\n",
       "      <td>0.002240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.137813</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.188220</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.137680</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.188198</td>\n",
       "      <td>0.002320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.137575</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.188185</td>\n",
       "      <td>0.002289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.137431</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.188228</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.137330</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.188207</td>\n",
       "      <td>0.002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.137266</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.188241</td>\n",
       "      <td>0.002333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.137159</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.188263</td>\n",
       "      <td>0.002415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.137034</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.188246</td>\n",
       "      <td>0.002338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.136882</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.188228</td>\n",
       "      <td>0.002363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.136820</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.188185</td>\n",
       "      <td>0.002420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.136679</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.188190</td>\n",
       "      <td>0.002430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.136637</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.188224</td>\n",
       "      <td>0.002434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.136532</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.188233</td>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.136417</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.136283</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.188203</td>\n",
       "      <td>0.002438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.136155</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.188147</td>\n",
       "      <td>0.002429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.136076</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.188198</td>\n",
       "      <td>0.002481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.135947</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.188185</td>\n",
       "      <td>0.002521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.135889</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.188276</td>\n",
       "      <td>0.002536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.135744</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.188237</td>\n",
       "      <td>0.002485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.135663</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.188164</td>\n",
       "      <td>0.002423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.135537</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.188160</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.135445</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.188121</td>\n",
       "      <td>0.002455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.213419         0.003642         0.220864        0.002292\n",
       "1            0.206908         0.003902         0.214756        0.002054\n",
       "2            0.204342         0.002058         0.213219        0.000707\n",
       "3            0.202497         0.001670         0.211242        0.001318\n",
       "4            0.201245         0.001235         0.210544        0.001329\n",
       "5            0.200197         0.001212         0.209876        0.001262\n",
       "6            0.199526         0.001348         0.208912        0.000577\n",
       "7            0.198771         0.001173         0.208550        0.000778\n",
       "8            0.197932         0.001041         0.207882        0.000807\n",
       "9            0.197547         0.000607         0.207710        0.000922\n",
       "10           0.196884         0.000598         0.207271        0.000885\n",
       "11           0.196230         0.000530         0.206900        0.001113\n",
       "12           0.195826         0.000609         0.206680        0.001143\n",
       "13           0.195235         0.000462         0.206116        0.001158\n",
       "14           0.194792         0.000623         0.205914        0.001319\n",
       "15           0.194234         0.000661         0.205561        0.001290\n",
       "16           0.193712         0.000760         0.205087        0.001166\n",
       "17           0.193344         0.000816         0.204768        0.001259\n",
       "18           0.192753         0.000804         0.204544        0.001185\n",
       "19           0.192227         0.000587         0.204122        0.000928\n",
       "20           0.191666         0.000456         0.203937        0.001055\n",
       "21           0.191278         0.000576         0.203648        0.001073\n",
       "22           0.190869         0.000621         0.203347        0.001203\n",
       "23           0.190461         0.000517         0.202942        0.001305\n",
       "24           0.189962         0.000549         0.202597        0.001243\n",
       "25           0.189516         0.000607         0.202287        0.001309\n",
       "26           0.189312         0.000573         0.201951        0.001340\n",
       "27           0.188756         0.000657         0.201826        0.001517\n",
       "28           0.188434         0.000720         0.201348        0.001582\n",
       "29           0.188088         0.000686         0.201391        0.001531\n",
       "..                ...              ...              ...             ...\n",
       "345          0.138505         0.001057         0.188203        0.002103\n",
       "346          0.138505         0.001034         0.188224        0.002136\n",
       "347          0.138329         0.001019         0.188164        0.002082\n",
       "348          0.138194         0.001071         0.188147        0.002074\n",
       "349          0.138078         0.001001         0.188151        0.002164\n",
       "350          0.138011         0.001028         0.188181        0.002217\n",
       "351          0.137940         0.001048         0.188233        0.002240\n",
       "352          0.137813         0.001049         0.188220        0.002300\n",
       "353          0.137680         0.001090         0.188198        0.002320\n",
       "354          0.137575         0.001111         0.188185        0.002289\n",
       "355          0.137431         0.001117         0.188228        0.002354\n",
       "356          0.137330         0.001102         0.188207        0.002373\n",
       "357          0.137266         0.001131         0.188241        0.002333\n",
       "358          0.137159         0.001144         0.188263        0.002415\n",
       "359          0.137034         0.001110         0.188246        0.002338\n",
       "360          0.136882         0.001040         0.188228        0.002363\n",
       "361          0.136820         0.001021         0.188185        0.002420\n",
       "362          0.136679         0.001055         0.188190        0.002430\n",
       "363          0.136637         0.001086         0.188224        0.002434\n",
       "364          0.136532         0.001025         0.188233        0.002402\n",
       "365          0.136417         0.001049         0.188203        0.002488\n",
       "366          0.136283         0.001026         0.188203        0.002438\n",
       "367          0.136155         0.000990         0.188147        0.002429\n",
       "368          0.136076         0.001026         0.188198        0.002481\n",
       "369          0.135947         0.001065         0.188185        0.002521\n",
       "370          0.135889         0.001093         0.188276        0.002536\n",
       "371          0.135744         0.001192         0.188237        0.002485\n",
       "372          0.135663         0.001185         0.188164        0.002423\n",
       "373          0.135537         0.001206         0.188160        0.002435\n",
       "374          0.135445         0.001226         0.188121        0.002455\n",
       "\n",
       "[375 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=10,\n",
    "    min_child_weight=5,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=8,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(train_features, label=train_targets)\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the colsample_bytree and subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "Run {} best score:  0.8125942197527674\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    " 'subsample':[i/10.0 for i in range(6,10)] \n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=375,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test4,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch4.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch4.best_params_)\n",
    "print('Run {} best score: ', gsearch4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'colsample_bytree': 0.95, 'subsample': 0.85}\n",
      "Run {} best score:  0.8129732523581858\n"
     ]
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'colsample_bytree':[0.85, 0.9, 0.95],\n",
    " 'subsample':[0.85, 0.9, 0.95] \n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=375,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test5,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch5.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch5.best_params_)\n",
    "print('Run {} best score: ', gsearch5.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'reg_alpha': 1e-05}\n",
      "Run {} best score:  0.8130206314338632\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=375,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.95,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test6,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch6.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch6.best_params_)\n",
    "print('Run {} best score: ', gsearch6.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run {} best param:  {'reg_alpha': 1e-05}\n",
      "Run {} best score:  0.8130206314338632\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=375,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.95,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                        param_grid = param_test7,\n",
    "                        scoring='accuracy',n_jobs=16,\n",
    "                        cv=five_folds)\n",
    "gsearch7.fit(train_features,train_targets)\n",
    "print('Run {} best param: ', gsearch7.best_params_)\n",
    "print('Run {} best score: ', gsearch7.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 81.2788%\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=375,\n",
    "        max_depth=10,\n",
    "        min_child_weight=5,\n",
    "        gamma=0,\n",
    "        subsample=0.85,\n",
    "        colsample_bytree=0.95,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        reg_alpha=1e-5,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0\n",
    "    )\n",
    "model = xgb.fit(train_features, train_targets)\n",
    "y_pred = model.predict(test_features)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test_targets, y_pred)\n",
    "print('Accuracy %.4f%%' % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
