{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/potusvn/Projects/kickstarter_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/potusvn/.conda/envs/kickstarter/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import models, optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from os.path import join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/data_quantile_transform/train_quantile_transform.csv', encoding='latin1', low_memory=True)\n",
    "test = pd.read_csv('data/data_quantile_transform/test_quantile_transform.csv', encoding='latin1', low_memory=True)\n",
    "val = pd.read_csv('data/data_quantile_transform/val_quantile_transform.csv', encoding='latin1', low_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185736, 222)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total = pd.concat([train, val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46434, 222)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_x = train_total.drop(['success'], axis=1)\n",
    "train_total_y = train_total.success\n",
    "\n",
    "test_x = test.drop(['success'], axis=1)\n",
    "test_y = test.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Encode the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network to encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 32)                7104      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 221)               7293      \n",
      "=================================================================\n",
      "Total params: 14,397\n",
      "Trainable params: 14,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 232170 samples, validate on 99502 samples\n",
      "Epoch 1/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.1672 - acc: 0.9155 - val_loss: -0.3097 - val_acc: 0.9341\n",
      "Epoch 2/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3224 - acc: 0.9365 - val_loss: -0.3322 - val_acc: 0.9394\n",
      "Epoch 3/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3405 - acc: 0.9408 - val_loss: -0.3449 - val_acc: 0.9418\n",
      "Epoch 4/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3495 - acc: 0.9424 - val_loss: -0.3515 - val_acc: 0.9430\n",
      "Epoch 5/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3549 - acc: 0.9435 - val_loss: -0.3559 - val_acc: 0.9440\n",
      "Epoch 6/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3587 - acc: 0.9444 - val_loss: -0.3591 - val_acc: 0.9447\n",
      "Epoch 7/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3613 - acc: 0.9450 - val_loss: -0.3612 - val_acc: 0.9451\n",
      "Epoch 8/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3630 - acc: 0.9453 - val_loss: -0.3627 - val_acc: 0.9454\n",
      "Epoch 9/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3642 - acc: 0.9455 - val_loss: -0.3637 - val_acc: 0.9456\n",
      "Epoch 10/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3651 - acc: 0.9457 - val_loss: -0.3643 - val_acc: 0.9457\n",
      "Epoch 11/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3656 - acc: 0.9458 - val_loss: -0.3648 - val_acc: 0.9458\n",
      "Epoch 12/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3661 - acc: 0.9459 - val_loss: -0.3652 - val_acc: 0.9459\n",
      "Epoch 13/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3664 - acc: 0.9459 - val_loss: -0.3656 - val_acc: 0.9459\n",
      "Epoch 14/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3667 - acc: 0.9459 - val_loss: -0.3658 - val_acc: 0.9459\n",
      "Epoch 15/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3669 - acc: 0.9460 - val_loss: -0.3659 - val_acc: 0.9460\n",
      "Epoch 16/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3670 - acc: 0.9460 - val_loss: -0.3660 - val_acc: 0.9460\n",
      "Epoch 17/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3671 - acc: 0.9460 - val_loss: -0.3661 - val_acc: 0.9460\n",
      "Epoch 18/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3671 - acc: 0.9460 - val_loss: -0.3661 - val_acc: 0.9460\n",
      "Epoch 19/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3672 - acc: 0.9460 - val_loss: -0.3662 - val_acc: 0.9460\n",
      "Epoch 20/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3672 - acc: 0.9460 - val_loss: -0.3662 - val_acc: 0.9460\n",
      "Epoch 21/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3672 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 22/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 23/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 24/1000\n",
      "232170/232170 [==============================] - 8s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 25/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 26/1000\n",
      "232170/232170 [==============================] - 8s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 27/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 28/1000\n",
      "232170/232170 [==============================] - 8s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3663 - val_acc: 0.9460\n",
      "Epoch 29/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 30/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3673 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 31/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 32/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 33/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 34/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 35/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 36/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 37/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 38/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 39/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 40/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 41/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 42/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 43/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 44/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 45/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 46/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 47/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3664 - val_acc: 0.9460\n",
      "Epoch 48/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3674 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 49/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 50/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 51/1000\n",
      "232170/232170 [==============================] - 8s 36us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 52/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 54/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 55/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 56/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 57/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 58/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 59/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 60/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 61/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 62/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 63/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 64/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 65/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 66/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 67/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 68/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 69/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 70/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 71/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 72/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 73/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 74/1000\n",
      "232170/232170 [==============================] - 9s 38us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 75/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 76/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 77/1000\n",
      "232170/232170 [==============================] - 9s 37us/step - loss: -0.3675 - acc: 0.9460 - val_loss: -0.3665 - val_acc: 0.9460\n",
      "Epoch 78/1000\n",
      "189440/232170 [=======================>......] - ETA: 1s - loss: -0.3676 - acc: 0.9460"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3fef7c7d1e19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;32m~/.conda/envs/kickstarter/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/kickstarter/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    185\u001b[0m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/.conda/envs/kickstarter/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kickstarter/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = 221\n",
    "encode_dim = 32\n",
    "model = models.Sequential()\n",
    "# Hidden - Layers\n",
    "model.add(Dense(encode_dim, activation=\"relu\", input_shape=(221,)))\n",
    "# Output- Layer\n",
    "model.add(Dense(input_dim, activation='sigmoid'))\n",
    "model.summary()\n",
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# Define some callbacks\n",
    "callbacks_func = []\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=50, verbose=0, mode='max')\n",
    "mcp_save = ModelCheckpoint('encode.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "callbacks_func = [mcp_save, earlyStopping]\n",
    "          \n",
    "results = model.fit(\n",
    "    train_total_x, train_total_x,\n",
    "    epochs=1000,\n",
    "    batch_size=512,\n",
    "    callbacks = callbacks_func,\n",
    "    validation_data=(test_x, test_x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.Sequential([\n",
    "    Dense(32, input_dim=221, activation=\"relu\")]) # first number is output_dim\n",
    "\n",
    "#set weights of the first layer\n",
    "encoder.set_weights(model.layers[0].get_weights())\n",
    "\n",
    "#compile it after setting the weights\n",
    "encoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_encode_x = encoder.predict(train_total_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encode_x = encoder.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_x_new = pd.np.column_stack([train_total_x, train_total_encode_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_new = pd.np.column_stack([test_x, test_encode_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test with new encoder data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_total_x_new[:int(len(train_total_x_new)*0.8), :]\n",
    "val_x = train_total_x_new[int(len(train_total_x_new)*0.8):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_total_y[:int(len(train_total_x_new)*0.8)]\n",
    "val_y = train_total_y[int(len(train_total_x_new)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, optimizer=optimizers.Adam(lr=0.01), batch_size=512, drop_out=0.0, l2_val=0, file_name=None):\n",
    "    model = models.Sequential()\n",
    "    # Hidden - Layers\n",
    "    for idx, layer in enumerate(hidden_layers):\n",
    "        if idx == 0:\n",
    "            model.add(Dense(layer, activation=\"relu\", input_shape=(253,), kernel_regularizer=l2(l2_val)))\n",
    "            model.add(Dropout(drop_out, noise_shape=None, seed=None))\n",
    "        else:\n",
    "            model.add(Dense(layer, activation=\"relu\", kernel_regularizer=l2(l2_val)))\n",
    "            model.add(Dropout(drop_out, noise_shape=None, seed=None))\n",
    "    # Output- Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    # compiling the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    # Define some callbacks\n",
    "    callbacks_func = []\n",
    "    earlyStopping = EarlyStopping(monitor='val_acc', patience=50, verbose=0, mode='max')\n",
    "    if file_name is None:\n",
    "        callbacks_func = [earlyStopping]\n",
    "    else:\n",
    "        mcp_save = ModelCheckpoint(file_name+'.hdf5', save_best_only=True, monitor='val_acc', mode='max')\n",
    "        callbacks_func = [mcp_save, earlyStopping]\n",
    "    results = model.fit(\n",
    "        train_x, train_y,\n",
    "        epochs=1000,\n",
    "        batch_size=batch_size,\n",
    "        callbacks = callbacks_func,\n",
    "        validation_data=(val_x, val_y)\n",
    "    )\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 20)                5080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,566\n",
      "Trainable params: 5,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 185736 samples, validate on 46434 samples\n",
      "Epoch 1/1000\n",
      "185736/185736 [==============================] - 2s 12us/step - loss: 0.6225 - acc: 0.6396 - val_loss: 0.4849 - val_acc: 0.7665\n",
      "Epoch 2/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4913 - acc: 0.7549 - val_loss: 0.4594 - val_acc: 0.7711\n",
      "Epoch 3/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4654 - acc: 0.7710 - val_loss: 0.4423 - val_acc: 0.7816\n",
      "Epoch 4/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4549 - acc: 0.7766 - val_loss: 0.4452 - val_acc: 0.7801\n",
      "Epoch 5/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4499 - acc: 0.7792 - val_loss: 0.4394 - val_acc: 0.7812\n",
      "Epoch 6/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4466 - acc: 0.7814 - val_loss: 0.4351 - val_acc: 0.7842\n",
      "Epoch 7/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4442 - acc: 0.7824 - val_loss: 0.4428 - val_acc: 0.7774\n",
      "Epoch 8/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4421 - acc: 0.7838 - val_loss: 0.4325 - val_acc: 0.7852\n",
      "Epoch 9/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4421 - acc: 0.7836 - val_loss: 0.4336 - val_acc: 0.7830\n",
      "Epoch 10/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4407 - acc: 0.7834 - val_loss: 0.4338 - val_acc: 0.7844\n",
      "Epoch 11/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4380 - acc: 0.7851 - val_loss: 0.4327 - val_acc: 0.7853\n",
      "Epoch 12/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4389 - acc: 0.7844 - val_loss: 0.4366 - val_acc: 0.7816\n",
      "Epoch 13/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4394 - acc: 0.7840 - val_loss: 0.4314 - val_acc: 0.7852\n",
      "Epoch 14/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4372 - acc: 0.7843 - val_loss: 0.4366 - val_acc: 0.7802\n",
      "Epoch 15/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4377 - acc: 0.7839 - val_loss: 0.4308 - val_acc: 0.7846\n",
      "Epoch 16/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4369 - acc: 0.7843 - val_loss: 0.4326 - val_acc: 0.7862\n",
      "Epoch 17/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4363 - acc: 0.7855 - val_loss: 0.4303 - val_acc: 0.7841\n",
      "Epoch 18/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4359 - acc: 0.7847 - val_loss: 0.4302 - val_acc: 0.7854\n",
      "Epoch 19/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4356 - acc: 0.7855 - val_loss: 0.4313 - val_acc: 0.7850\n",
      "Epoch 20/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4358 - acc: 0.7851 - val_loss: 0.4300 - val_acc: 0.7850\n",
      "Epoch 21/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4345 - acc: 0.7860 - val_loss: 0.4311 - val_acc: 0.7858\n",
      "Epoch 22/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4347 - acc: 0.7856 - val_loss: 0.4344 - val_acc: 0.7833\n",
      "Epoch 23/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4339 - acc: 0.7862 - val_loss: 0.4313 - val_acc: 0.7852\n",
      "Epoch 24/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4349 - acc: 0.7855 - val_loss: 0.4302 - val_acc: 0.7856\n",
      "Epoch 25/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4341 - acc: 0.7858 - val_loss: 0.4297 - val_acc: 0.7846\n",
      "Epoch 26/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4336 - acc: 0.7861 - val_loss: 0.4301 - val_acc: 0.7861\n",
      "Epoch 27/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4334 - acc: 0.7867 - val_loss: 0.4284 - val_acc: 0.7863\n",
      "Epoch 28/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4333 - acc: 0.7871 - val_loss: 0.4297 - val_acc: 0.7863\n",
      "Epoch 29/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4333 - acc: 0.7861 - val_loss: 0.4274 - val_acc: 0.7872\n",
      "Epoch 30/1000\n",
      "185736/185736 [==============================] - 2s 9us/step - loss: 0.4329 - acc: 0.7866 - val_loss: 0.4290 - val_acc: 0.7849\n",
      "Epoch 31/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4330 - acc: 0.7871 - val_loss: 0.4324 - val_acc: 0.7857\n",
      "Epoch 32/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4325 - acc: 0.7872 - val_loss: 0.4272 - val_acc: 0.7871\n",
      "Epoch 33/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4322 - acc: 0.7872 - val_loss: 0.4296 - val_acc: 0.7856\n",
      "Epoch 34/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4317 - acc: 0.7867 - val_loss: 0.4287 - val_acc: 0.7867\n",
      "Epoch 35/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4320 - acc: 0.7872 - val_loss: 0.4319 - val_acc: 0.7833\n",
      "Epoch 36/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4327 - acc: 0.7870 - val_loss: 0.4285 - val_acc: 0.7867\n",
      "Epoch 37/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4314 - acc: 0.7869 - val_loss: 0.4288 - val_acc: 0.7855\n",
      "Epoch 38/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4324 - acc: 0.7863 - val_loss: 0.4265 - val_acc: 0.7874\n",
      "Epoch 39/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4320 - acc: 0.7874 - val_loss: 0.4312 - val_acc: 0.7864\n",
      "Epoch 40/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4325 - acc: 0.7873 - val_loss: 0.4316 - val_acc: 0.7842\n",
      "Epoch 41/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4309 - acc: 0.7872 - val_loss: 0.4326 - val_acc: 0.7862\n",
      "Epoch 42/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4307 - acc: 0.7881 - val_loss: 0.4287 - val_acc: 0.7861\n",
      "Epoch 43/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4315 - acc: 0.7874 - val_loss: 0.4283 - val_acc: 0.7866\n",
      "Epoch 44/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4316 - acc: 0.7877 - val_loss: 0.4297 - val_acc: 0.7851\n",
      "Epoch 45/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4300 - acc: 0.7878 - val_loss: 0.4301 - val_acc: 0.7867\n",
      "Epoch 46/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4317 - acc: 0.7875 - val_loss: 0.4286 - val_acc: 0.7849\n",
      "Epoch 47/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4312 - acc: 0.7885 - val_loss: 0.4288 - val_acc: 0.7844\n",
      "Epoch 48/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4320 - acc: 0.7866 - val_loss: 0.4270 - val_acc: 0.7883\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4310 - acc: 0.7875 - val_loss: 0.4287 - val_acc: 0.7865\n",
      "Epoch 50/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4321 - acc: 0.7870 - val_loss: 0.4296 - val_acc: 0.7838\n",
      "Epoch 51/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4305 - acc: 0.7880 - val_loss: 0.4274 - val_acc: 0.7868\n",
      "Epoch 52/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4310 - acc: 0.7880 - val_loss: 0.4292 - val_acc: 0.7849\n",
      "Epoch 53/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4304 - acc: 0.7882 - val_loss: 0.4264 - val_acc: 0.7877\n",
      "Epoch 54/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4316 - acc: 0.7874 - val_loss: 0.4271 - val_acc: 0.7872\n",
      "Epoch 55/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4309 - acc: 0.7874 - val_loss: 0.4305 - val_acc: 0.7872\n",
      "Epoch 56/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4303 - acc: 0.7879 - val_loss: 0.4314 - val_acc: 0.7852\n",
      "Epoch 57/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4307 - acc: 0.7875 - val_loss: 0.4286 - val_acc: 0.7846\n",
      "Epoch 58/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4309 - acc: 0.7873 - val_loss: 0.4286 - val_acc: 0.7857\n",
      "Epoch 59/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4314 - acc: 0.7874 - val_loss: 0.4277 - val_acc: 0.7873\n",
      "Epoch 60/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4303 - acc: 0.7879 - val_loss: 0.4271 - val_acc: 0.7877\n",
      "Epoch 61/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4302 - acc: 0.7882 - val_loss: 0.4382 - val_acc: 0.7774\n",
      "Epoch 62/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4310 - acc: 0.7875 - val_loss: 0.4305 - val_acc: 0.7857\n",
      "Epoch 63/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7892 - val_loss: 0.4263 - val_acc: 0.7882\n",
      "Epoch 64/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4310 - acc: 0.7884 - val_loss: 0.4278 - val_acc: 0.7867\n",
      "Epoch 65/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4299 - acc: 0.7886 - val_loss: 0.4286 - val_acc: 0.7872\n",
      "Epoch 66/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4294 - acc: 0.7884 - val_loss: 0.4281 - val_acc: 0.7851\n",
      "Epoch 67/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4301 - acc: 0.7884 - val_loss: 0.4256 - val_acc: 0.7884\n",
      "Epoch 68/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4304 - acc: 0.7874 - val_loss: 0.4262 - val_acc: 0.7866\n",
      "Epoch 69/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4305 - acc: 0.7882 - val_loss: 0.4270 - val_acc: 0.7874\n",
      "Epoch 70/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4309 - acc: 0.7874 - val_loss: 0.4264 - val_acc: 0.7884\n",
      "Epoch 71/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4309 - acc: 0.7870 - val_loss: 0.4263 - val_acc: 0.7872\n",
      "Epoch 72/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4304 - acc: 0.7874 - val_loss: 0.4259 - val_acc: 0.7878\n",
      "Epoch 73/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4303 - acc: 0.7874 - val_loss: 0.4273 - val_acc: 0.7880\n",
      "Epoch 74/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4299 - acc: 0.7892 - val_loss: 0.4259 - val_acc: 0.7879\n",
      "Epoch 75/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4298 - acc: 0.7889 - val_loss: 0.4265 - val_acc: 0.7882\n",
      "Epoch 76/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4301 - acc: 0.7883 - val_loss: 0.4274 - val_acc: 0.7876\n",
      "Epoch 77/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4294 - acc: 0.7887 - val_loss: 0.4272 - val_acc: 0.7878\n",
      "Epoch 78/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4300 - acc: 0.7882 - val_loss: 0.4307 - val_acc: 0.7862\n",
      "Epoch 79/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4302 - acc: 0.7879 - val_loss: 0.4287 - val_acc: 0.7871\n",
      "Epoch 80/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4293 - acc: 0.7876 - val_loss: 0.4265 - val_acc: 0.7892\n",
      "Epoch 81/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4297 - acc: 0.7881 - val_loss: 0.4270 - val_acc: 0.7878\n",
      "Epoch 82/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4299 - acc: 0.7878 - val_loss: 0.4278 - val_acc: 0.7876\n",
      "Epoch 83/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4296 - acc: 0.7877 - val_loss: 0.4270 - val_acc: 0.7887\n",
      "Epoch 84/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4294 - acc: 0.7879 - val_loss: 0.4252 - val_acc: 0.7878\n",
      "Epoch 85/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4287 - acc: 0.7886 - val_loss: 0.4265 - val_acc: 0.7888\n",
      "Epoch 86/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4303 - acc: 0.7874 - val_loss: 0.4290 - val_acc: 0.7861\n",
      "Epoch 87/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4298 - acc: 0.7884 - val_loss: 0.4271 - val_acc: 0.7881\n",
      "Epoch 88/1000\n",
      "185736/185736 [==============================] - 2s 9us/step - loss: 0.4301 - acc: 0.7878 - val_loss: 0.4265 - val_acc: 0.7886\n",
      "Epoch 89/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4301 - acc: 0.7871 - val_loss: 0.4265 - val_acc: 0.7867\n",
      "Epoch 90/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4297 - acc: 0.7873 - val_loss: 0.4261 - val_acc: 0.7873\n",
      "Epoch 91/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4295 - acc: 0.7876 - val_loss: 0.4278 - val_acc: 0.7865\n",
      "Epoch 92/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4288 - acc: 0.7882 - val_loss: 0.4271 - val_acc: 0.7860\n",
      "Epoch 93/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4292 - acc: 0.7892 - val_loss: 0.4271 - val_acc: 0.7869\n",
      "Epoch 94/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4289 - acc: 0.7878 - val_loss: 0.4286 - val_acc: 0.7869\n",
      "Epoch 95/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4295 - acc: 0.7878 - val_loss: 0.4298 - val_acc: 0.7866\n",
      "Epoch 96/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4288 - acc: 0.7880 - val_loss: 0.4285 - val_acc: 0.7853\n",
      "Epoch 97/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4305 - acc: 0.7873 - val_loss: 0.4259 - val_acc: 0.7887\n",
      "Epoch 98/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4290 - acc: 0.7876 - val_loss: 0.4261 - val_acc: 0.7884\n",
      "Epoch 99/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7879 - val_loss: 0.4280 - val_acc: 0.7859\n",
      "Epoch 100/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4293 - acc: 0.7882 - val_loss: 0.4261 - val_acc: 0.7889\n",
      "Epoch 101/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4290 - acc: 0.7878 - val_loss: 0.4290 - val_acc: 0.7863\n",
      "Epoch 102/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4297 - acc: 0.7871 - val_loss: 0.4293 - val_acc: 0.7894\n",
      "Epoch 103/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4297 - acc: 0.7890 - val_loss: 0.4261 - val_acc: 0.7883\n",
      "Epoch 104/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4289 - acc: 0.7878 - val_loss: 0.4252 - val_acc: 0.7885\n",
      "Epoch 105/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4300 - acc: 0.7877 - val_loss: 0.4294 - val_acc: 0.7867\n",
      "Epoch 106/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4295 - acc: 0.7880 - val_loss: 0.4252 - val_acc: 0.7888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4292 - acc: 0.7881 - val_loss: 0.4304 - val_acc: 0.7847\n",
      "Epoch 108/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4290 - acc: 0.7884 - val_loss: 0.4276 - val_acc: 0.7873\n",
      "Epoch 109/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4292 - acc: 0.7879 - val_loss: 0.4306 - val_acc: 0.7832\n",
      "Epoch 110/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4300 - acc: 0.7876 - val_loss: 0.4274 - val_acc: 0.7875\n",
      "Epoch 111/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4291 - acc: 0.7884 - val_loss: 0.4253 - val_acc: 0.7886\n",
      "Epoch 112/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4295 - acc: 0.7885 - val_loss: 0.4281 - val_acc: 0.7886\n",
      "Epoch 113/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7883 - val_loss: 0.4265 - val_acc: 0.7888\n",
      "Epoch 114/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4289 - acc: 0.7885 - val_loss: 0.4262 - val_acc: 0.7883\n",
      "Epoch 115/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4295 - acc: 0.7876 - val_loss: 0.4258 - val_acc: 0.7874\n",
      "Epoch 116/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4293 - acc: 0.7878 - val_loss: 0.4339 - val_acc: 0.7827\n",
      "Epoch 117/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7875 - val_loss: 0.4265 - val_acc: 0.7887\n",
      "Epoch 118/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4289 - acc: 0.7881 - val_loss: 0.4274 - val_acc: 0.7858\n",
      "Epoch 119/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4295 - acc: 0.7880 - val_loss: 0.4259 - val_acc: 0.7881\n",
      "Epoch 120/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4282 - acc: 0.7892 - val_loss: 0.4266 - val_acc: 0.7880\n",
      "Epoch 121/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4293 - acc: 0.7881 - val_loss: 0.4280 - val_acc: 0.7847\n",
      "Epoch 122/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4297 - acc: 0.7881 - val_loss: 0.4259 - val_acc: 0.7875\n",
      "Epoch 123/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4293 - acc: 0.7880 - val_loss: 0.4291 - val_acc: 0.7849\n",
      "Epoch 124/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4300 - acc: 0.7882 - val_loss: 0.4268 - val_acc: 0.7877\n",
      "Epoch 125/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4287 - acc: 0.7889 - val_loss: 0.4255 - val_acc: 0.7887\n",
      "Epoch 126/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7886 - val_loss: 0.4263 - val_acc: 0.7879\n",
      "Epoch 127/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4289 - acc: 0.7881 - val_loss: 0.4259 - val_acc: 0.7889\n",
      "Epoch 128/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7888 - val_loss: 0.4258 - val_acc: 0.7884\n",
      "Epoch 129/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4290 - acc: 0.7877 - val_loss: 0.4279 - val_acc: 0.7871\n",
      "Epoch 130/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4296 - acc: 0.7882 - val_loss: 0.4288 - val_acc: 0.7859\n",
      "Epoch 131/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4298 - acc: 0.7878 - val_loss: 0.4250 - val_acc: 0.7887\n",
      "Epoch 132/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4289 - acc: 0.7882 - val_loss: 0.4265 - val_acc: 0.7883\n",
      "Epoch 133/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4283 - acc: 0.7882 - val_loss: 0.4257 - val_acc: 0.7907\n",
      "Epoch 134/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4296 - acc: 0.7885 - val_loss: 0.4239 - val_acc: 0.7889\n",
      "Epoch 135/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4286 - acc: 0.7885 - val_loss: 0.4299 - val_acc: 0.7844\n",
      "Epoch 136/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4291 - acc: 0.7883 - val_loss: 0.4267 - val_acc: 0.7874\n",
      "Epoch 137/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4288 - acc: 0.7882 - val_loss: 0.4311 - val_acc: 0.7856\n",
      "Epoch 138/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4292 - acc: 0.7877 - val_loss: 0.4261 - val_acc: 0.7879\n",
      "Epoch 139/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4287 - acc: 0.7881 - val_loss: 0.4266 - val_acc: 0.7887\n",
      "Epoch 140/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4292 - acc: 0.7875 - val_loss: 0.4279 - val_acc: 0.7883\n",
      "Epoch 141/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4298 - acc: 0.7882 - val_loss: 0.4274 - val_acc: 0.7865\n",
      "Epoch 142/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4282 - acc: 0.7887 - val_loss: 0.4271 - val_acc: 0.7885\n",
      "Epoch 143/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7884 - val_loss: 0.4277 - val_acc: 0.7874\n",
      "Epoch 144/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4293 - acc: 0.7878 - val_loss: 0.4264 - val_acc: 0.7900\n",
      "Epoch 145/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4290 - acc: 0.7889 - val_loss: 0.4263 - val_acc: 0.7887\n",
      "Epoch 146/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7881 - val_loss: 0.4293 - val_acc: 0.7867\n",
      "Epoch 147/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7883 - val_loss: 0.4261 - val_acc: 0.7885\n",
      "Epoch 148/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4288 - acc: 0.7885 - val_loss: 0.4254 - val_acc: 0.7902\n",
      "Epoch 149/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4286 - acc: 0.7881 - val_loss: 0.4290 - val_acc: 0.7887\n",
      "Epoch 150/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7884 - val_loss: 0.4261 - val_acc: 0.7886\n",
      "Epoch 151/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7886 - val_loss: 0.4291 - val_acc: 0.7852\n",
      "Epoch 152/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4282 - acc: 0.7891 - val_loss: 0.4252 - val_acc: 0.7901\n",
      "Epoch 153/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4281 - acc: 0.7883 - val_loss: 0.4285 - val_acc: 0.7878\n",
      "Epoch 154/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7883 - val_loss: 0.4250 - val_acc: 0.7883\n",
      "Epoch 155/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4286 - acc: 0.7884 - val_loss: 0.4246 - val_acc: 0.7899\n",
      "Epoch 156/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7885 - val_loss: 0.4297 - val_acc: 0.7869\n",
      "Epoch 157/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4285 - acc: 0.7882 - val_loss: 0.4249 - val_acc: 0.7893\n",
      "Epoch 158/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4290 - acc: 0.7884 - val_loss: 0.4278 - val_acc: 0.7854\n",
      "Epoch 159/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7889 - val_loss: 0.4268 - val_acc: 0.7865\n",
      "Epoch 160/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4286 - acc: 0.7883 - val_loss: 0.4247 - val_acc: 0.7887\n",
      "Epoch 161/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4285 - acc: 0.7884 - val_loss: 0.4253 - val_acc: 0.7886\n",
      "Epoch 162/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4284 - acc: 0.7882 - val_loss: 0.4257 - val_acc: 0.7885\n",
      "Epoch 163/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4287 - acc: 0.7876 - val_loss: 0.4256 - val_acc: 0.7890\n",
      "Epoch 164/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4288 - acc: 0.7881 - val_loss: 0.4244 - val_acc: 0.7893\n",
      "Epoch 165/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7884 - val_loss: 0.4253 - val_acc: 0.7880\n",
      "Epoch 166/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4285 - acc: 0.7889 - val_loss: 0.4310 - val_acc: 0.7864\n",
      "Epoch 167/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4295 - acc: 0.7888 - val_loss: 0.4252 - val_acc: 0.7876\n",
      "Epoch 168/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4301 - acc: 0.7880 - val_loss: 0.4266 - val_acc: 0.7884\n",
      "Epoch 169/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4288 - acc: 0.7873 - val_loss: 0.4278 - val_acc: 0.7858\n",
      "Epoch 170/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7880 - val_loss: 0.4278 - val_acc: 0.7875\n",
      "Epoch 171/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7880 - val_loss: 0.4273 - val_acc: 0.7864\n",
      "Epoch 172/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7882 - val_loss: 0.4277 - val_acc: 0.7861\n",
      "Epoch 173/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4287 - acc: 0.7879 - val_loss: 0.4271 - val_acc: 0.7874\n",
      "Epoch 174/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4285 - acc: 0.7882 - val_loss: 0.4255 - val_acc: 0.7895\n",
      "Epoch 175/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4284 - acc: 0.7885 - val_loss: 0.4261 - val_acc: 0.7879\n",
      "Epoch 176/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4291 - acc: 0.7878 - val_loss: 0.4303 - val_acc: 0.7870\n",
      "Epoch 177/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4282 - acc: 0.7885 - val_loss: 0.4266 - val_acc: 0.7887\n",
      "Epoch 178/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4285 - acc: 0.7886 - val_loss: 0.4284 - val_acc: 0.7876\n",
      "Epoch 179/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4297 - acc: 0.7879 - val_loss: 0.4254 - val_acc: 0.7885\n",
      "Epoch 180/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4280 - acc: 0.7879 - val_loss: 0.4270 - val_acc: 0.7883\n",
      "Epoch 181/1000\n",
      "185736/185736 [==============================] - 2s 10us/step - loss: 0.4283 - acc: 0.7884 - val_loss: 0.4281 - val_acc: 0.7871\n",
      "Epoch 182/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4290 - acc: 0.7878 - val_loss: 0.4250 - val_acc: 0.7893\n",
      "Epoch 183/1000\n",
      "185736/185736 [==============================] - 2s 11us/step - loss: 0.4280 - acc: 0.7886 - val_loss: 0.4278 - val_acc: 0.7881\n"
     ]
    }
   ],
   "source": [
    "model_encode, result_encode = build_model(hidden_layers=[20,15,10], drop_out=0.1, l2_val=0.0001, optimizer=optimizers.Adam(lr=0.001),file_name='model_encode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_without_train(hidden_layers, drop_out=0.0, l2_val=0, optimizer=optimizers.Adam(lr=0.000001)):\n",
    "    model = models.Sequential()\n",
    "    # Hidden - Layers\n",
    "    for idx, layer in enumerate(hidden_layers):\n",
    "        if idx == 0:\n",
    "            model.add(Dense(layer, activation=\"relu\", input_shape=(253,), kernel_regularizer=l2(l2_val)))\n",
    "            model.add(Dropout(drop_out, noise_shape=None, seed=None))\n",
    "        else:\n",
    "            model.add(Dense(layer, activation=\"relu\", kernel_regularizer=l2(l2_val)))\n",
    "            model.add(Dropout(drop_out, noise_shape=None, seed=None))\n",
    "    # Output- Layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 20)                5080      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,566\n",
      "Trainable params: 5,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = build_model_without_train(hidden_layers=[20,15,10], drop_out=0.1, l2_val=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_weights('model_encode.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99502/99502 [==============================] - 4s 42us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4230642681863329, 0.790335872646974]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate(test_x_new, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kickstarter)",
   "language": "python",
   "name": "kickstarter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
